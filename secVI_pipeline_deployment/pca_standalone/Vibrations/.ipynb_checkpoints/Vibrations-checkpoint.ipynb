{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib \n",
    "import matplotlib.pyplot as plt\n",
    "import datetime as dt\n",
    "from datetime import datetime \n",
    "from datetime import date\n",
    "from tqdm import tqdm \n",
    "from dateutil.relativedelta import relativedelta\n",
    "import scipy.fftpack\n",
    "from scipy import signal\n",
    "from scipy.signal import find_peaks\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import DataSet from Hard Drive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensor_checkup(day, month, year, sensor_num = 'S6.1.1'): \n",
    "    sensor_num = sensor_num.upper()\n",
    "    if 'S' in sensor_num:\n",
    "        lut = pd.read_csv('values/S_tabel_'+str(year)+'_'+str(month) + '.csv')\n",
    "        sensor_name = sensor_num.replace('.',\"\")\n",
    "        if lut[sensor_name].values[day-1] == 1 & lut['Presence'].values[day-1] == 1: \n",
    "            return 1 \n",
    "        else:  \n",
    "            return 0\n",
    "    else: \n",
    "        print(\"Inserted Sensor Name is Invlid\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To upload one day \n",
    "def day_import(sensor_num = 'S6.1.3', gg = None, directory = None):\n",
    "    ''' \n",
    "    start and finish date should be entered as 'date(2019,6,20)'. \n",
    "    '''\n",
    "    ds = []\n",
    "    pflag = 0\n",
    "    y,m,d = gg.strftime('%Y,%m,%d').split(\",\")\n",
    "    check = sensor_checkup(day = int(d), month = int(m), year = int(y) ,sensor_num = sensor_num)\n",
    "    day = y + '-' + m + '-' + d\n",
    "    if check == 1: \n",
    "        ds = pd.read_parquet(directory + 'ss335-acc'+day+sensor_num.replace('.','')+ '.parquet')\n",
    "        pflag = 1\n",
    "        return ds, pflag\n",
    "    elif check == 0: \n",
    "        ds = None\n",
    "        pflag = 0\n",
    "        return ds, pflag\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload a period of time \n",
    "def store_days(start = None, end = None, sensor_num = 's.611', directory = None): \n",
    "    '''\n",
    "    Start and end day should be inserted as date(yy,mm,dd)\n",
    "    '''\n",
    "    \n",
    "    output = {}\n",
    "    flag = {}\n",
    "    period = relativedelta(end, start).days\n",
    "    period += 1\n",
    "    print(str(period) + ' number of days are asked to be uploaded')\n",
    "    for gg in tqdm(pd.date_range(start = start, end = end)): \n",
    "        y,m,d = gg.strftime('%Y,%m,%d').split(\",\")\n",
    "        output[str(int(d))],flag[str(int(d))]  = day_import(sensor_num= sensor_num, gg = gg, \n",
    "                         directory=directory)\n",
    "    return output, flag\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store summary of the uploaded data\n",
    "def summary_store(pflag, start = None, end = None): \n",
    "    moment = datetime.now()\n",
    "    mth = moment.month\n",
    "    d = moment.day\n",
    "    h = moment.hour\n",
    "    mit = moment.minute\n",
    "    period = relativedelta(end, start).days\n",
    "    period += 1 \n",
    "    y,m,d = start.strftime('%Y,%m,%d').split(\",\")\n",
    "    file = open('summary of month_'+str(int(m))+' which stored at_'+str(d)+'_'+str(h)+'_'+str(mit)+'time.txt', 'w')\n",
    "\n",
    "    for i in range(period): \n",
    "        comp = pflag[str(int(d)+ i)]\n",
    "        if comp == 0: \n",
    "            file.write('day ' + str(int(d)+ i) + ' does NOT belong in the dataset for requiered sensor\\n')\n",
    "        elif comp == 1: \n",
    "            file.write('day ' + str(int(d)+ i) + ' is present in dataset\\n')\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate discussed period\n",
    "def conc_dict(ds, pflag, start = None, end = None, axis = 'z'): \n",
    "    output = {}\n",
    "    start = start\n",
    "    end = end\n",
    "    y,m,d = start.strftime('%Y,%m,%d').split(\",\")\n",
    "\n",
    "    size = relativedelta(end, start).days\n",
    "    size += 1\n",
    "    for i in range(size):\n",
    "        comp = pflag[str(int(d)+i)]\n",
    "        if comp == 0: \n",
    "            continue\n",
    "        elif comp == 1:\n",
    "            if output == {}:\n",
    "                output[axis] = ds[str(int(d)+i)][axis].values\n",
    "                output['ts'] = ds[str(int(d)+i)]['ts'].values\n",
    "            else: \n",
    "                output[axis] = np.append(output[axis], ds[str(int(d)+i)][axis].values) \n",
    "                output['ts'] = np.append(output['ts'], ds[str(int(d)+i)]['ts'].values)   \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather All together \n",
    "def import_data(sens = 's611', axis = 'z', start = None, end = None, directory = None): \n",
    "    # First we store days in a dictionary (call from hard disk)\n",
    "    pure_ds , flags = store_days(start = start, end = end, \n",
    "               sensor_num = sens, directory = directory)\n",
    "    # Then we feed it to write a summary of what we upload: \n",
    "    #summary_store(flags, start = start, end = end)\n",
    "    # Then we concatenate the axis we want in to one array: \n",
    "    print(\"Start to Merging Days of selected axis:\")\n",
    "    selected_ds = conc_dict(pure_ds, flags, start = start, end = end,\n",
    "                            axis = axis)\n",
    "    print(\"End of Merging\\nSelected axis is: \" + axis)\n",
    "    # Return selected dataset for an axis + pure dataset\n",
    "    return pure_ds, selected_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 number of days are asked to be uploaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:05<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to Merging Days of selected axis:\n",
      "End of Merging\n",
      "Selected axis is: z\n",
      "Uploading time is: 0:00:07.526368 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = datetime.now()\n",
    "pure_may, sel_may = import_data(sens = 's613', axis = 'z', \n",
    "                                  start = date(2019,5,10), end = date(2019,5,12),\n",
    "                                  directory = 'values/')\n",
    "tac = datetime.now()\n",
    "print('Uploading time is: ' + str(tac - tic) + ' seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Window Data For different Purposes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_window(ds=None, axis = None, start = None, end= None): \n",
    "    ''' \n",
    "    Start and Finish should be inserted as dt.datetime(year,month,day,hour,minute,second).timestamp(\n",
    "    axis also should be entered in this way => axis = 'z'\n",
    "    '''\n",
    "    output = {} \n",
    "    #define the upper boundary of the window\n",
    "    output[axis] = ds[axis][(ds['ts']/1000)<end]\n",
    "    output['ts'] = ds['ts'][(ds['ts']/1000)<end]\n",
    "    #define the lower boundary of the window\n",
    "    output[axis] = output[axis][(output['ts']/1000)>start]\n",
    "    output['ts'] = output['ts'][(output['ts']/1000)>start]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "st1 = dt.datetime(2019,5,10,0,0,1).timestamp()\n",
    "end1 = dt.datetime(2019,5,12,0,0,0).timestamp()\n",
    "test = time_window(ds = sel_may, axis = 'z', start=st1, end=end1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = (1*2.5)*2**-15\n",
    "values = test['z']\n",
    "scaled_values = values * conv    # To Verify the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import serial\n",
    "import time\n",
    "\n",
    "\"\"\" \n",
    "IMPORTANT: if your port is already open uncomment the following line. \n",
    "\"\"\"\n",
    "#com.close()   \n",
    "\n",
    "\"\"\"\n",
    "Defining the com to send the data via UART\n",
    "\"\"\"\n",
    "com = serial.Serial(\"com5\", baudrate= 115200, \n",
    "                    parity = serial.PARITY_NONE, \n",
    "                    stopbits = serial.STOPBITS_ONE, \n",
    "                    bytesize = serial.EIGHTBITS,\n",
    "                    timeout = 100000)\n",
    "#com.open()\n",
    "com.isOpen()\n",
    "i = 0\n",
    "\"\"\"\n",
    "For consistency of comparisons in all 3 dimensions: \n",
    "    - interval of 1 sec is 9600 - 9700 \n",
    "    - interval of 2 sec is 9600 - 9800\n",
    "    - interval of 5 sec is 9500 - 10000\n",
    "\"\"\"\n",
    "for aray in values[9500:10000]:\n",
    "    i  = i+1\n",
    "    # To have 8 bits of data for each value, following reshaping is vital \n",
    "    if aray >0: \n",
    "        string = '+' + str(aray) +'\\r\\n' \n",
    "    else:\n",
    "        string = str(aray) + '\\r\\n' \n",
    "    com.write((string).encode('ascii'))\n",
    "    time.sleep(0.05) # to avoid data loss\n",
    "com.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MSE Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_reconst = lambda data,eigenvector: np.linalg.multi_dot([data,eigenvector, eigenvector.T])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful FoM to evaluate reconstruction of PCA \n",
    "RSNR = lambda x, xr: np.sum(x**2, axis=0)/np.sum((x - xr)**2, axis=0)\n",
    "ARSNR = lambda x, xr: np.mean(RSNR(x, xr))\n",
    "RSNRdB = lambda x, xr: 10*np.log10(RSNR(x, xr))\n",
    "ARSNRdB = lambda x, xr: 10*np.log10(ARSNR(x, xr))\n",
    "### ==================================================\n",
    "mse_w = lambda x, xr: np.mean((x - xr)**2, axis=0)\n",
    "mse = lambda x, xr: np.mean(mse_w(x, xr))\n",
    "energy = lambda x: np.sum(x**2, axis=0)/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_val = pd.read_csv('eigen_values/eigenVal_1sec.csv')  # here put the name of the desired file that you want\n",
    "eigen_val = eigen_val.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_shape(ds, input_dim):\n",
    "    d = input_dim\n",
    "    n = ds.shape[0]\n",
    "    n0 = n // input_dim\n",
    "    data = ds[:n0*input_dim]\n",
    "    data = data.reshape((n0,d)).T\n",
    "    mean_numero = np.mean(data, axis = 0)\n",
    "    data_mean = data - np.mean(data, axis = 0)\n",
    "    energy = np.sum(data_mean**2, axis = 0)/input_dim\n",
    "    return data, data_mean, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input dimension should be inserted as integer e.g 1,2,5,etc.\n",
    "ds_py, ds_mean, ds_eg = time_shape(scaled_values, 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  96   97   98  445  446  902  903  904  905 1039 1040 1041 1042 1043\n",
      " 1044 1045 1046 1085 1086 1087 1090 1091 1117 1118 1119]\n"
     ]
    }
   ],
   "source": [
    "a = np.where(ds_eg > 3.125e-06) \n",
    "print(a[0][:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.preprocessing\n",
    "\"\"\"\n",
    "Corresponding number for former defined intervals are: \n",
    "    - 1 sec is 96\n",
    "    - 2 sec is 48\n",
    "    - 5 sec is 19\n",
    "\"\"\"\n",
    "scaled_data = sklearn.preprocessing.scale(ds_mean[:,96], with_mean=True, with_std=True, copy=True)\n",
    "reco = pca_reconst(scaled_data,eigen_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = (reco - scaled_data)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6358322155436077\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(sub))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF CODE "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
